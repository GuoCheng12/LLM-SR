
"""
Find the mathematical function skeleton that represents acceleration in a damped nonlinear oscillator system with driving force, given data on time, position, and velocity.
The data includes observational noise and uncertainty, simulating real experimental scenarios in physical oscillations. The noise is modeled as group-level additive Gaussian noise, where each group represents an independent measurement batch with its own noise scale. Observational error is directly added to the acceleration (a), while propagation error arises from noise in position (x) and velocity (v) transmitting through the function's partial derivatives to affect a. The total uncertainty for each point is calculated as sigma_total = sqrt(sigma_a^2 + (|partial f / partial x| * sigma_x)^2 + (|partial f / partial v| * sigma_v)^2), where sigma_a, sigma_x, and sigma_v are the noise scales for a, x, and v respectively. This uncertainty is used in weighted evaluation to prioritize low-uncertainty points and ensure robustness. The function form can only contain differentiable mathematical terms. When generating skeletons, prioritize forms that are robust to uncertainty: avoid high-derivative terms (e.g., high powers or exponentials with large coefficients) that amplify propagation error, as they can lead to instability in noisy data. Focus on physically interpretable terms like sinusoidal driving forces, nonlinear damping, and position-velocity interactions to mimic damped oscillators.
"""

import torch
import numpy as np
import logging
from tqdm import tqdm
#Initialize parameters
MAX_NPARAMS = 10
PRAMS_INIT = [torch.nn.Parameter(torch.tensor(1.0)) for _ in range(MAX_NPARAMS)]

@evaluate.run
def evaluate(data: dict) -> tuple[float, list]:
    """Evaluate the equation on noisy data with dual supervision using GPU.

    Args:
        data (dict): Contains 't', 'x', 'v', 'a', 'sigma_x', 'sigma_v', 'sigma_a', 'sigma_total'.

    Returns:
        tuple[float, list]: Negative combined loss and optimized parameters, or (None, None) if invalid.
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"Running evaluate on {device}")

    t = torch.tensor(data['t'], device=device, requires_grad=False)
    x = torch.tensor(data['datax'], device=device, requires_grad=True)
    v = torch.tensor(data['datav'], device=device, requires_grad=True)
    a = torch.tensor(data['dataa'], device=device)
    sigma_x = torch.tensor(data['sigma_x'], device=device)
    sigma_v = torch.tensor(data['sigma_v'], device=device)
    sigma_a = torch.tensor(data['sigma_a'], device=device)
    sigma_total = torch.tensor(data['sigma_total'], device=device)

    LR = 1e-4
    N_ITERATIONS = 10000
    PRAMS_INIT = [torch.nn.Parameter(torch.tensor(1.0, device=device)) for _ in range(MAX_NPARAMS)]

    class Model(torch.nn.Module):
        def __init__(self):
            super(Model, self).__init__()
            self.params = torch.nn.ParameterList(PRAMS_INIT)

        def forward(self, t, x, v):
            return equation(t, x, v, self.params)

    model = Model().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    for _ in tqdm(range(N_ITERATIONS)):
        optimizer.zero_grad()
        y_pred = model(t, x, v)
        if torch.any(torch.isnan(y_pred)) or torch.any(torch.isinf(y_pred)):
            logging.error("NaN or Inf detected in y_pred")
            return None, None
        # Value loss with weighting by 1/sigma_total^2
        weights = 1.0 / (sigma_total ** 2 + 1e-8)  # Avoid division by zero
        value_loss = torch.mean(weights * (y_pred - a) ** 2)
        
        # Uncertainty loss
        grad_outputs = torch.ones_like(y_pred)
        try:
            gradients = torch.autograd.grad(y_pred, (x, v), grad_outputs=grad_outputs, create_graph=True)
            grad_x, grad_v = gradients[0], gradients[1]
            if torch.any(torch.isnan(grad_x)) or torch.any(torch.isnan(grad_v)):
                logging.error("NaN detected in gradients")
                return None, None
        except RuntimeError as e:
            logging.error(f"Gradient computation failed: {e}")
            return None, None
        
        pred_uncertainty = torch.sqrt((grad_x.abs() * sigma_x)**2 + (grad_v.abs() * sigma_v)**2 + 1e-8)
        # Expected propagation uncertainty = sqrt(max(0, sigma_total^2 - sigma_a^2))
        expected_prop = torch.sqrt((sigma_total**2 - sigma_a**2).clamp(min=0.0))
        uncertainty_loss = torch.mean((pred_uncertainty - expected_prop) ** 2)
        
        loss = value_loss + uncertainty_loss
        if torch.isnan(loss) or torch.isinf(loss):
            logging.error("NaN or Inf detected in loss")
            return None, None
        loss.backward()
        optimizer.step()

    if torch.isnan(loss) or torch.isinf(loss):
        return None, None
    params_values = [p.item() for p in model.params]
    return -loss.item(), params_values


@equation.evolve
def equation(t: torch.Tensor, x: torch.Tensor, v: torch.Tensor, params: torch.nn.ParameterList) -> torch.Tensor:
    """ Mathematical function for acceleration in a damped nonlinear oscillator

    Args:
        t (torch.Tensor): time.
        x (torch.Tensor): observations of current position.
        v (torch.Tensor): observations of velocity.
        params (torch.nn.ParameterList): List of numeric constants or parameters to be optimized

    Return:
        torch.Tensor: acceleration as the result of applying the mathematical function to the inputs.
    """
    dv = params[0] * t  +  params[1] * x +  params[2] * v + params[3]
    return dv